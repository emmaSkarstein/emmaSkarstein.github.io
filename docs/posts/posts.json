[
  {
    "path": "posts/2022-06-10-isba-2022-poster-supplement/",
    "title": "ISBA 2022 poster supplement",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-06-10",
    "categories": [],
    "contents": "\n\nABSTRACT\nMeasurement error and missing values in covariates are inescapable in\nany discipline that deals with data, and both problems have received\nconsiderable attention. Despite this, many applied scientists are still\nnot routinely accounting for varying types of measurement error in their\ncovariates.\nHere we describe a joint Bayesian framework that simultaneously\naccounts for measurement error and missing data, by using the fact that\nmissing data can be interpreted as an extreme case of missing data. In\naddition, we show how to account for Berkson measurement error in the\nsame framework, which provides a convenient setup for modelling Berkson\nmeasurement error, classical measurement error, missing data, or any\ncombination of these in the same or different covariates.\nBy using integrated nested Laplace approximations (INLA) we show how\npotentially complex Bayesian hierarchical models can be implemented\nefficiently. The approach is applicable to a wide variety of regression\nmodels, some of which will be exemplified in this paper using both\nsimulated and real data.\nTODO: Adjust this to the poster presentation\nTHE\nMOST GENERAL MODEL: Missing data, classical measurement error and\nBerkson measurement error\nTODO: Equations and text from the paper\nEXAMPLES\nPoster design\nMy poster design is completely copied from this Psycho poster (link).\nI have also been very inspired by Mike Morrison’s #betterposter\ncampaign, and would definitely recommend checking out his videos on how\nto make better academic posters in less time, here is one of them: https://www.youtube.com/watch?v=WBjhxjWDiHw. Of course,\nMike Morrison’s point is to help researchers spend less time on their\nposters. But I really like fiddling with the design of my poster, and so\nI tried to adapt his principles while simultaneously trying to copy the\ndesign of the movie poster. I think vintage movie posters or\nadvertisements are actually great models for how we could design\nacademic posters, as they grab your attention with impactful and\nattractive colors and fonts, while at the same time not being too\nover-crowded and often very minimalistic. I think this use of fonts,\ncolors and simplistic figures or icons can be a great guide, and copying\na movie poster makes it more of an interesting challenge rather than a\nfrustration.\nI made the poster in Power Point, using google fonts when I couldn’t\nfind a good font match in the default fonts. The equations are made\nusing eulerfont in Latex and appropriate font and background color, and\nthen screen-shotted into the poster.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-10T17:24:01+02:00",
    "input_file": "isba-2022-poster-supplement.knit.md"
  },
  {
    "path": "posts/2022-05-02-testing/",
    "title": "Testing",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-05-02",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-05-02T16:49:12+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-using-spin-to-create-manageable-supplementary-material/",
    "title": "Using spin to create manageable supplementary material",
    "description": "Good supplementary material means a lot more than just uploading your R scripts. Rmarkdown allows us to easily provide comments and explanations of our code, but it means more files to manage -- perhaps you already have the code in an R script and are manually copying it into the .Rmd-file. With knitr's spin function, you can automatically convert your R script to an Rmarkdown file, leading to easier code-sharing and ultimately making your research easier to re-do. Here, I share some of the tricks I have learned about how to make clean, user-friendly supplementary material using spin.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-05-02",
    "categories": [],
    "contents": "\nThe `knitr::spin()`` function takes any r script and turns it into a html or pdf document without any further modifications. You do this either by clicking the little notebook symbol in the toolbar above your script, or by pressing shift + command + K (the same shortcut as you use to knit). However, if you want to, you may use the syntax for Roxygen comments to add normal text to the compiled document, as well as a YAML frontmatter and anything else you would to in a regular Rmarkdown document. You can read more about the details of basic spinning here: https://bookdown.org/yihui/rmarkdown-cookbook/spin.html\nIn this post I want to share some things I’ve learned from using spin to create the supplementary material for my paper.\nFile paths\nChild files\nWorkflow\n\n\n\n",
    "preview": {},
    "last_modified": "2022-05-02T16:51:59+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-08-power-usage-project/",
    "title": "Power consumption project",
    "description": "Through elhub.no I have access to all the power consumption data from my apartment. I have made the data available in an R package, and this is a brief look at it!",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2021-11-08",
    "categories": [],
    "contents": "\n(This post was first made in January 2021, I’m just reposting now that I have this blog thing going)\nWelcome to my power usage project! Through elhub I have downloaded (and will continue to download) monthly data sets containing the hourly power usage in my apartment. I made an R-package that contains all the data plus some useful functions. Here I will explore some visualisations of the data.\nMy goal is to produce visualizations of these data that are easy to interpret for people without any particular statistical background. I this document I will be trying out different things and showing a bit of the process, so this is not meant to be a perfect finished product, rather a kind of documentation of the process.\n\n\nShow code\n\n#devtools::install_github(\"emmaSkarstein/power.usage.analysis\")\nlibrary(power.usage.analysis)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(hrbrthemes)\nlibrary(ggridges)\nlibrary(patchwork)\nlibrary(jcolors)\nlibrary(ggpubr)\ntheme_set(theme_bw())\n\n\n\nFirst, let us load the data and look at the structure of it.\n\n\nShow code\n\ndata <- clean_and_prepare(power_data)\nhead(data)\n\n\n                     tid forbruk       date month weekday year\n...1 2019-01-14 00:00:00   0.338 2019-01-13     1  Monday 2019\n...2 2019-01-14 01:00:00   1.047 2019-01-14     1  Monday 2019\n...3 2019-01-14 02:00:00   0.327 2019-01-14     1  Monday 2019\n...4 2019-01-14 03:00:00   1.475 2019-01-14     1  Monday 2019\n...5 2019-01-14 04:00:00   2.791 2019-01-14     1  Monday 2019\n...6 2019-01-14 05:00:00   3.286 2019-01-14     1  Monday 2019\n         hour\n...1 00:00:00\n...2 01:00:00\n...3 02:00:00\n...4 03:00:00\n...5 04:00:00\n...6 05:00:00\n\nSo our data cosists of the hourly power consumption, as well as columns giving the year, date, month, weekday and hour for each observation. Now I want to explore some visualizations.\nBasic plot\nLet’s first try a simple plot of all the observations.\n\n\nShow code\n\np_simple <- ggplot(data, aes(x = tid, y = forbruk)) + \n  geom_line() +\n  ylab(\"Power usage\") +\n  xlab(\"\")\np_simple\n\n\n\n\nThis is a mess. While we do see some general tendencies that are interesting (like maximum consumption varying seasonally), it is difficult to tell what is going on in all the noise. It is also worth noting the really low values during August of 2020. This was due to some error with our power measuring device, and we had to have a guy from the company come fix it. I won’t be handling this in any special way, for now I will just treat them along with the rest of the data. It is worth noting that the values have not been set to zero or NA, instead they are just unrealistically low. If these kinds of problems with the measuring device was a common problem, this would be really frustrating, as there is no way of knowing then if the measurement is actually that low or if there is an error (although appearently we didn’t get charged less for this period, since they knew something was wrong they instead extrapolated from the previous correct values).\nAverage per weekday - Pitfalls of the boxplot\nAnother option is to try to group the data by some time period by summing or averaging it. For instance, we can look at the power usage for each weekday. For this, we might be interested in looking at the average power consuption per day, which could easily be calculated. However, just calculating the average, without showing any information about the spread or variation in the observations, is not very good. And when we also have the option to display things graphically instead of just giving summary statistics, a boxplot is a common choice. In this case, a boxplot would look like this:\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk)) +\n  geom_boxplot(fill = \"hotpink3\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  annotate(\"text\", x = 1, y = c(-0.1, 0.4, 0.75, 1.9, 4.5, 6), label = c(\"Minimum\", \"25th percentile\", \"Median\", \"75th percentile\", \"Maximum\", \"Outliers\"), size = 2.2, fontface = \"bold\", color= \"darkcyan\")\n\n\n\n\nThe boxplot displays a lot of information. The first thing I look at is the bold line in the middle of the orange box, which indicates the median (the 50th percentile) of the values in this group. The edges of the box indicate the 25th and 75th percentiles (25th percentile means the value that 25% of the observations lie below, and 75% lie above, and similarly for the 75th percentile). The ends of the “whiskers” show the minimum and maximum value of the data, excluding the outliers, and then the outliers are plotted explicitly.\n\nThe box plot (or plots with range-bars) was introduced by the data visualization specialist Mary Eleanor Spear, first in her book Charting statistics in 1952.\n\nFrom the above plot, it really doesn’t look to me like there are any real differences in power usage between each weekday. I would be ok with leaving it like this, but boxplots are a bit controversial, since they may easily show the same box plot for observations of very different distributions. See this nice article for a further discussion of this. There are some ways to fix this, however. A simple fix to make sure we aren’t hiding any important information about the distribution of the data is to plot a jittered scatterplot on top. That would look like this for us:\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk)) +\n  geom_boxplot(fill = \"darkorange\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  geom_jitter(color=\"black\", size=0.4, alpha=0.9)\n\n\n\n\nThis is quite transparent, but in our case it looks chaotic since we have so many observations. It also does’t solve the problem that the boxplot is not immediately obvious if you have never seen one before, which I would say is a major disadvantage when visualizing data for most audiences.\nAn alternative to the box-plot is the violin plot. Personally, I’m not a fan of the violin plot because I initially found the fact that it is symmetric around a center line to be a bit confusing. If I want the plot to be really easy to interpret, I find the ridge plot to be superior to both the boxplot and violin plot. Here is how it looks using the same data as above:\n\n\nShow code\n\nggplot(data, aes(x = forbruk, y = weekday)) +\n  geom_density_ridges(alpha = 0.8, fill = \"hotpink3\") +\n  theme_ridges() +\n  ylab(\"\") +\n  xlab(\"Power usage\")\n\n\n\n\n—-some interpretation of this—-\nAs a last point I just want to compare the boxplot, violin plot and ridge plot here for the reader, because I think preferences vary and it is interesting seeing the different options (note that I’ve flipped the axes on the two first so they are the same direction as the ridge plot).\n\n\nShow code\n\np_box <- ggplot(data, aes(y = weekday, x = forbruk)) +\n  geom_boxplot(fill = \"darkorange2\") +\n  xlab(\"\") +\n  ylab(\"\")\n\np_violin <-ggplot(data, aes(y = weekday, x = forbruk)) +\n  geom_violin(fill = \"hotpink3\") +\n  xlab(\"\") +\n  ylab(\"\") + \n  theme(axis.text.y = element_blank())\n\np_ridges <- ggplot(data, aes(x = forbruk, y = weekday)) +\n  geom_density_ridges(alpha = 0.8, fill = \"darkgreen\") +\n  ylab(\"\") +\n  xlab(\"\") +\n  theme(axis.text.y = element_blank())\n\n(p_box + p_violin + xlab(\"Power usage\") + p_ridges) \n\n\n\n\nOkay, it pretty much looks like the distribution is the same for each week day, maybe not surprisingly, although I might have expected the consumption to be a bit higher during the week-end. Really kind of boring.\nDid the pandemic affect power usage for each weekday?\nHowever, during most of 2020 we worked from home. Could it be that this is affecting the results, and that weekday differences were larger in 2019? Let’s investigate.\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk, fill = year)) +\n  geom_boxplot() +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nThere are some differences here, but I really don’t think we can say for certain that there is a meaningful difference between the power consumption in 2019 and 2020 for any of the weekdays. This makes sense, because although we were probably more outside of the appartment in 2019, usually there would be at least one of us at home regardless, so the situation is not very different.\nTotal per month\nInspired by the weekday plots, I want to examine if the total monthly power consumption has changed between 2019 and 2020. Instead of looking at the median, average or some other measure per month, I will simply look at the total power consumption per month in 2019, and separately in 2020, since a month is a continuous stretch of time. Had I been looking at monthly consumption not separated by year, however, I would have calculated the average total monthly consumption across those years. Maybe that will be interesting once I have data from more than just two years.\nAnyway – here is the total power usage per month for 2019 and 2020. This is also the plot that our power provider shows me on their webpage, and I feel like it is particularly useful for just seeing how much we use. Of course – some months are longer than others, so it is not completely fair to compare between the months, but it certainly gives a rough impression.\n\n\nShow code\n\nyear_month <- data %>%\n  group_by(month, year) %>%\n  summarise(sum_forb = sum(forbruk))\n\nggplot(year_month, aes(x = month, y = sum_forb)) +\n  geom_col(aes(fill = year), color = \"black\", position = \"dodge\") +\n  scale_fill_jcolors(palette = \"pal6\") +\n  ylab(\"Total power usage per month\") +\n  xlab(\"\")\n\n\n\n\nHere we can sort of see the seasonal trend, and it seems very reasonable to me. I find it interesting that March is the month with the highest power consumption, I would not have guessed that in advance.\nDaily usage\nThere is another resolution between our first plot and the monthly total plot in the previous section: it might also be interesting to look at the total daily consumption. I expect this will be slightly smoother than the first plot, and it may be interesting to see the time-series day-by-day instead of grouped by month like above.\n\n\nShow code\n\ndaily <- data %>%\n  group_by(date) %>%\n  summarise(sum_forb = sum(forbruk))\n\nggplot(daily, aes(x = date, y = sum_forb)) +\n  geom_point(size = 0.5) +\n  geom_line(alpha = 0.5, color = \"hotpink4\") +\n  ylab(\"Power usage\") +\n  xlab(\"\")\n\n\n\n\nHere we see more of the variation between different days in the same month. I would really have liked to know what happened on those peak days.\nUsage throughout the day\nAs a final plot, I would like to know how the power usage is distributed throughout the day. First just a simple box plot:\n\n\nShow code\n\nggplot(data, aes(x = hour, y = forbruk)) +\n  geom_boxplot(fill = \"hotpink3\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nI also want to try a fancier, less detailed version that represents time periods with higher usage as larger areas. I have split the day into 3-hour intervals (should I use a finer resolution?), and the below plot shows the total power consumption in each time period. (Would it be better with a donut to represent the day as a cycle?) (would be nice to show the times on the plot somehow)\n\n\nShow code\n\n# Adding grouped time column\ndata <- data %>% mutate(grouped_time = \n                          factor(floor(as.numeric(substr(hour, start = 1, stop = 2))/3)))\n\nggplot(data, aes(x = 1, y = -forbruk, fill = grouped_time)) +\n  geom_col() +\n  scale_fill_brewer(labels = c(\"00:00 - 02:00\", \"03:00 - 05:00\", \n                                 \"06:00 - 08:00\", \"09:00 - 11:00\", \n                                 \"12:00 - 14:00\", \"15:00 - 17:00\",\n                                \"18:00 - 20:00\", \"21:00 - 23:00\"),\n                    palette = \"BrBG\") +\n  theme_transparent() +\n  coord_flip() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-08-power-usage-project/power-usage-project_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-11-09T17:39:00+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-28-ways-of-visualizing-missing-data/",
    "title": "Ways of visualizing missing data",
    "description": "When recently looking at a data set with a lot of missing data I tried out a few different ways of quickly summarizing the missingness in the different variables. Here is a brief guide to the visualizations I found the most useful!",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\nFor this demonstration, we will borrow datasets from the package mice.\n\n\nShow code\n\nlibrary(mice)\nlibrary(tidyverse)\nlibrary(patchwork) # Combining plots\nlibrary(showtext) # Font\nlibrary(ggthemes) # Color palettes (I use the canva palettes here)\nlibrary(naniar) # Upset plots for missing values in ggplot2\nlibrary(eulerr) # Euler diagrams, proportional to size\nlibrary(ggforce) # Ellipses in ggplot\nlibrary(gt) # Great tables\nlibrary(gtExtras) # Extra stuff for the tables\nlibrary(scales) # For wrapping axis text\nlibrary(paletteer) # color palettes (I used a fish one)\n\n\n\nFirst example: Mammal sleep\nOur first data set is a set of various sleep characteristics of 62 mammals.\n\n\nShow code\n\ndata(mammalsleep)\n\ndata <- mammalsleep\n\nbetter_names <- c(Species = \"species\", Body_weight = \"bw\", Brain_weight = \"brw\", \n                  Slow_wave_sleep = \"sws\", Paradoxical_sleep = \"ps\", \n                  Total_sleep = \"ts\", Maximum_life_span = \"mls\", \n                  Gestation_time = \"gt\", Predation_index = \"pi\", \n                  Sleep_exposure_index = \"sei\", Overall_danger_index = \"odi\")\n\ndata <- data %>% rename(all_of(better_names))\nnames(data) <- gsub(\"_\", \" \", names(data), fixed=TRUE)\n\n\n# Font\nf1 <- \"Open Sans\"\nfont_add_google(name = f1, family = f1)\nshowtext_auto()\n\n# Colors\npal <- paletteer_d(\"fishualize::Halichoeres_radiatus\")\n\n\n\nDirect plotting of missingness: A rowplot\n\n\nShow code\n\nmissing_row.plot <- data %>%\n  mutate(id = row_number()) %>%\n  gather(-id, key = \"key\", value = \"val\") %>%\n  mutate(isna = is.na(val)) %>%\n  ggplot(aes(key, id, fill = isna)) +\n  geom_raster(alpha=0.82) +\n  scale_fill_manual(name = \"\",\n                    values = pal[c(1,3)],\n                    labels = c(\"Present\", \"Missing\")) +\n  scale_x_discrete(position = \"top\", labels = wrap_format(10), expand = c(0,0)) +\n  scale_y_continuous(breaks = c(1, seq(10, nrow(data), by = 10)), expand = c(0,0), trans = \"reverse\") +\n  labs(x = \"\",\n       y = \"Row Number\") +\n  theme_bw() +\n  theme(legend.position = \"top\",\n        text = element_text(size = 15, family = f1),\n        panel.grid = element_blank())\n\nmissing_row.plot\n\n\n\n\nSummary table\nNext, I just want a table that summarizes the number of missing observations in each variable. For this, I will first make a data frame with the counts and percentages of missing in each variable:\n\n\nmissing_count <- data %>% is.na %>% as.data.frame() %>% \n      map_int(sum) %>% as.data.frame() \nmissing_count$variable = rownames(missing_count)\nmissing_count <- missing_count %>% rename(count = \".\") %>% \n  mutate(percent = 100*count/nrow(data)) %>% \n  relocate(variable)\n\n\n\nThen we can easily make this into a nice table using the gt package.\n\n\nShow code\n\n# Table -----\ncolumns_with_missing <- missing_count %>% \n  filter(count > 0) %>% \n  dplyr::select(variable) %>% \n  as.matrix()\n\nmissing_table <- missing_count %>% \n  dplyr::filter(variable %in% columns_with_missing) %>% # select only columns that have missing data\n  rename(Variable = \"variable\", \"# missing\" = count, \"% missing\" = percent) %>% # rename columns\n  gt() %>% # table\n  gt_plt_bar_pct(column = \"% missing\", scaled = TRUE, fill = pal[4]) %>%  # make the percentage column into a barchart\n  tab_style(style = cell_text( # change font\n    font = google_font(f1)), \n    locations = list(cells_column_labels(everything()), \n                     cells_body(columns = c(1,2))))\n\nmissing_table\n\n\n\nVariable\n      # missing\n      % missing\n    Slow wave sleep\n14\nParadoxical sleep\n12\nTotal sleep\n4\nMaximum life span\n4\nGestation time\n4\n\n\nEuler diagram\nThe row-plot and table are both great for getting a quick overview of the data and the number of missing values. But especially with the table, we have no information about the interactions in the missingness, that is, are many of the missing values in the same row? We see this to some degree in the row-plot, but in this case we only have 62 observations. When the number of observations increases it becomes less clear when the missingness is in the same row. Toillustrate the overlaps in the missingness, I thought it would be illustrative with some kind of venn-diagram (I learned that the correct term for the type of plot that doesn’t show overlaps when the set is null is called an Euler diagram). I also wanted the size of the circles and overlaps to be proportional to the overlaps and number of missing observations. I found what I wanted in the package eulerr. There is a built-in-way to plot the resulting Euler diagrams, but I wanted to do it with ggplot2for a bit more freedom. It wasn’t too hard to extract the necessary numbers from the eulerr object (with good help from this vignette), and for plotting the ellipses themselves I use the ggforce package.\n\n\nShow code\n\n# Euler plot ------\neuler_mat <- data %>% is.na() %>% as.data.frame() %>% \n  dplyr::select(columns_with_missing[1:5]) \n\neuler_fit <- euler(euler_mat)\n\nellipses <- euler_fit$ellipses %>% mutate(variable = rownames(euler_fit$ellipses))\n\nmissing_euler <- ggplot(ellipses) +\n  geom_ellipse(aes(x0 = h, y0 = k, a = a, b = b, angle = phi, fill = variable), alpha = 0.5) +\n  scale_fill_manual(values = pal) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.title = element_blank())\n\nmissing_euler\n\n\n\n\nI really like the way this looks, but unfortunately it isn’t exact, especially when there are so few observations. For example, from this diagram it looks like there would be an observation that is missing the “Paradoxical sleep” measurement, but not the “Slow wave sleep”, due to the tiny un-overlapped sliver on the left. However, looking at the row-plot, we see that the set of animals missing values in “Paradoxical sleep”, is completely contained in the set of animals missing values in “Slow wave sleep”. The eulerr object gives us an overview over both the true counts in each set and the fitted values, and these could also be plotted on top of the circles, but I won’t do this here since there are so many intersections. Unfortunately, though the idea is fun, I don’t think this visualization will work very well in many cases.\n\n\neuler_fit$original.values[1:15]\n\n\n                    Slow wave sleep \n                                  0 \n                  Paradoxical sleep \n                                  0 \n                        Total sleep \n                                  0 \n                  Maximum life span \n                                  2 \n                     Gestation time \n                                  3 \n  Slow wave sleep&Paradoxical sleep \n                                  9 \n        Slow wave sleep&Total sleep \n                                  2 \n  Slow wave sleep&Maximum life span \n                                  0 \n     Slow wave sleep&Gestation time \n                                  0 \n      Paradoxical sleep&Total sleep \n                                  0 \nParadoxical sleep&Maximum life span \n                                  0 \n   Paradoxical sleep&Gestation time \n                                  0 \n      Total sleep&Maximum life span \n                                  0 \n         Total sleep&Gestation time \n                                  0 \n   Maximum life span&Gestation time \n                                  1 \n\nUpSet plot\nThere is another option that solves the problem of the impreciseness of the euler plot. I was first a little skeptical of this one just because I don’t think it is completely self-explanatory, and I think in most contexts, getting an overview of the missingness is something you want to do quick and dirty, and if visualizations are necessary you want them to be super intuitive. But this one is more precise than the Euler diagram and also shows the interactions, so to an audience that is already familiar with them (and maybe with some helpful annotations), I think it can be really useful. The plot is called an upset plot, and can be used as an alternative to Venn diagrams in other cases than just visualizing missingness. I use the implementation from the library naniar (which has several other useful functions for these types of things!)\n\n\nShow code\n\ngg_miss_upset(data, sets.bar.color = pal[1], main.bar.color = pal[4])\n\n\n\n\nThe bars on the left show the total number of missing values for each of the variables, and the vertical bars show the numbers missing in each intersection. My main complaint here is that although the documentation says that it returns a ggplot visualization, I don’t seem to be able to edit it using my typical ggplot ways, to change the color I instead had to use the arguments from UpSetR::upset.\nFor the project that motivated me to write this post, I made my own upset plot in ggplot2, but it is kind of hard-coded and the code is specific to that data. The approach in itself wasn’t too complicated though, basically you just convert the data to a data frame of the same size, but with true/false values indicating whether each observation is missing or not. Then it takes some manipulation to make the counts for each of the sets and interactions, and then the plots themselves are just standard bar charts. I also just used a dot-plot for the table-part of it, and then combined them all using patchwork (the package). If I’m able to generalize the code I will happily share it at a later point.\nAnother example\nAs another example, let’s look at this data set from mice with self-reported height and weight data from two studies, containing 2060 observations. A description of the data can be found by ?mice::selfreport.\n\n\nShow code\n\ndata(\"selfreport\")\n\ndata <- selfreport\n\n# Colors\npal <- paletteer_d(\"fishualize::Lutjanus_sebae\")\n\n\n\n\n\nShow code\n\nmissing_row.plot <- data %>%\n  mutate(id = row_number()) %>%\n  gather(-id, key = \"key\", value = \"val\") %>%\n  mutate(isna = is.na(val)) %>%\n  ggplot(aes(key, id, fill = isna)) +\n  geom_raster(alpha=0.82) +\n  scale_fill_manual(name = \"\",\n                    values = pal[c(1,4)],\n                    labels = c(\"Present\", \"Missing\")) +\n  scale_x_discrete(position = \"top\", labels = wrap_format(10), expand = c(0,0)) +\n  scale_y_continuous(breaks = c(1, seq(100, nrow(data), by = 100)), expand = c(0,0), trans = \"reverse\") +\n  labs(x = \"\",\n       y = \"Row Number\") +\n  theme_bw() +\n  theme(legend.position = \"top\",\n        text = element_text(size = 15, family = f1),\n        panel.grid = element_blank())\n\nmissing_row.plot\n\n\n\n\n\n\nShow code\n\nmissing_count <- data %>% is.na %>% as.data.frame() %>% \n      map_int(sum) %>% as.data.frame() \nmissing_count$variable = rownames(missing_count)\nmissing_count <- missing_count %>% rename(count = \".\") %>% \n  mutate(percent = 100*count/nrow(data)) %>% \n  relocate(variable)\n\n\n\n\n\nShow code\n\n# Table -----\ncolumns_with_missing <- missing_count %>% \n  filter(count > 0) %>% \n  dplyr::select(variable) %>% \n  as.matrix()\n\nmissing_table <- missing_count %>% \n  dplyr::filter(variable %in% columns_with_missing) %>% # select only columns that have missing data\n  rename(Variable = \"variable\", \"# missing\" = count, \"% missing\" = percent) %>% # rename columns\n  gt() %>% # table\n  gt_plt_bar_pct(column = \"% missing\", scaled = TRUE, fill = pal[4]) %>%  # make the percentage column into a barchart\n  tab_style(style = cell_text( # change font\n    font = google_font(f1)), \n    locations = list(cells_column_labels(everything()), \n                     cells_body(columns = c(1,2))))\n\nmissing_table\n\n\n\nVariable\n      # missing\n      % missing\n    hm\n803\nwm\n803\nprg\n1657\nedu\n1257\netn\n1257\nbm\n803\n\n\n\n\nShow code\n\n# Euler plot ------\neuler_mat <- data %>% is.na() %>% as.data.frame() %>% \n  dplyr::select(columns_with_missing[1:5]) \n\neuler_fit <- euler(euler_mat)\n\nellipses <- euler_fit$ellipses %>% mutate(variable = rownames(euler_fit$ellipses))\n\nmissing_euler <- ggplot(ellipses) +\n  geom_ellipse(aes(x0 = h, y0 = k, a = a, b = b, angle = phi, fill = variable), alpha = 0.5) +\n  scale_fill_manual(values = pal) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.title = element_blank())\n\nmissing_euler\n\n\n\n\n(Actually, it looks like eulerr just displays five sets, while we here have six variables with missing values, so this isn’t great.)\n\n\nShow code\n\ngg_miss_upset(data, sets.bar.color = pal[2], main.bar.color = pal[4])\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-10-28-ways-of-visualizing-missing-data/ways-of-visualizing-missing-data_files/figure-html5/figure.rowplot-1.png",
    "last_modified": "2021-11-08T15:04:30+01:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-28T14:56:41+02:00",
    "input_file": {}
  }
]
