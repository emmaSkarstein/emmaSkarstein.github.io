[
  {
    "path": "posts/2023-02-23-what-did-women-work-with-in-1920-a-data-visualization-case-study/",
    "title": "What did women work with in 1920?",
    "description": "In this post I'll be looking at the occupations of the 11992 women who registered to vote in Boston in 1920. Using `ggplot2` I'll be making a bar chart with a curved bar to display the different occupations.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2023-02-23",
    "categories": [],
    "contents": "\n\nContents\nGet the data and setting up\nConsiderations in grouping the occupations\nA preliminary bar chart\nA bent bar chart!\nAppendix: Some other exploration of the data\nBirth countries\nPopular and rare names\n\n\nI became aware of the city of Boston’s Mary Eliza project through the Data is Plural newsletter. The Mary Eliza project is transcribing 160 handwritten volumes of the General Registers of Women Voters from the City of Boston in 1920, and making all this data available to the public. This is such a rich and fascinating data set to look into! Here, I examine the reported jobs of these women, and through that I attempt to give some insight into what life was like for these women, more than 100 years ago.\nGet the data and setting up\nI’ll be using the tidyverse package:\n\n\nlibrary(tidyverse)\n\n\nWould you like to follow along? You can download the data yourself from the project website. Note that this dataset is updated continuously, so the dataset may have expanded since I downloaded it (January 18th 2023). I have the exact data I used on my GitHub repo, in case you want to make sure you have the same data. All the code for this project is also on GitHub.\nTo begin with, I’ll load the data and clean the column names.\n\n\nvoters_raw <- read.csv(\"https://raw.githubusercontent.com/emmaSkarstein/visualization_projects/master/Women_voters/womens-voter-registers.csv\")\nvoters <- voters_raw %>% \n  janitor::clean_names()\n\n\nConsiderations in grouping the occupations\nSo what I really want to look closer at for this project are the recorded occupations. We can do a quick count of the top ones:\n\n\noccupations <- voters %>% count(occupation, sort = TRUE) \nhead(occupations, 15)\n\n           occupation    n\n1           Housewife 3939\n2             At home  715\n3         Housekeeper  598\n4               Clerk  455\n5        Stenographer  330\n6           housewife  286\n7          Bookkeeper  218\n8             At Home  207\n9                None  207\n10          Housework  203\n11               <NA>  180\n12 Married, Housewife  158\n13            Teacher  156\n14          Secretary  154\n15              Nurse  143\n\nJust like with the countries, I already see some things I would like to clean up here, but we need to be careful to not make incorrect assumptions when working on this historical data. However, there are some things I think we can safely assume. Let’s print out a longer list of just the occupations without the counts to get a better idea of what we will need to do:\n\n\noccupations[1:100, 1]\n\n  [1] \"Housewife\"            \"At home\"             \n  [3] \"Housekeeper\"          \"Clerk\"               \n  [5] \"Stenographer\"         \"housewife\"           \n  [7] \"Bookkeeper\"           \"At Home\"             \n  [9] \"None\"                 \"Housework\"           \n [11] NA                     \"Married, Housewife\"  \n [13] \"Teacher\"              \"Secretary\"           \n [15] \"Nurse\"                \"H.W.\"                \n [17] \"Dressmaker\"           \"at home\"             \n [19] \"Saleslady\"            \"Telephone Operator\"  \n [21] \"Married\"              \"Single\"              \n [23] \"Married\\nHousewife\"   \"Laundress\"           \n [25] \"clerk\"                \"married, housewife\"  \n [27] \"Operator\"             \"Waitress\"            \n [29] \"housewife, married\"   \"Cashier\"             \n [31] \"Domestic\"             \"Milliner\"            \n [33] \"Matron\"               \"house-wife\"          \n [35] \"Seamstress\"           \"Stitcher\"            \n [37] \"Tel. operator\"        \"House-wife\"          \n [39] \"Student\"              \"Telephone operator\"  \n [41] \"Cook\"                 \"at Home\"             \n [43] \"Widow\"                \"Maid\"                \n [45] \"School teacher\"       \"Married, housewife\"  \n [47] \"Saleswoman\"           \"Forelady\"            \n [49] \"Book-keeper\"          \"Elevator operator\"   \n [51] \"none\"                 \"Book keeper\"         \n [53] \"housekeeper\"          \"Packer\"              \n [55] \"Typist\"               \"Machine operator\"    \n [57] \"stenographer\"         \"Artist\"              \n [59] \"Hairdresser\"          \"Inspector\"           \n [61] \"Social worker\"        \"unemployed\"          \n [63] \"Bookbinder\"           \"married, Housewife\"  \n [65] \"Buyer\"                \"Manager\"             \n [67] \"Box maker\"            \"House-keeper\"        \n [69] \"Librarian\"            \"nurse\"               \n [71] \"Physician\"            \"saleslady\"           \n [73] \"Accountant\"           \"bookkeeper\"          \n [75] \"Graduate nurse\"       \"house keeper\"        \n [77] \"School Teacher\"       \"dressmaker\"          \n [79] \"married\"              \"Musician\"            \n [81] \"Trained nurse\"        \"Bookeeper\"           \n [83] \"laundress\"            \"Storekeeper\"         \n [85] \"Boarding\"             \"Tailoress\"           \n [87] \"Bank clerk\"           \"Box Maker\"           \n [89] \"Companion\"            \"Housewife, Married\"  \n [91] \"Lodging house keeper\" \"Machine Operator\"    \n [93] \"Music teacher\"        \"Private secretary\"   \n [95] \"Retired\"              \"Shoe worker\"         \n [97] \"teacher\"              \"Auditor\"             \n [99] \"Brushmaker\"           \"Cherry picker\"       \n\nThere are some things to note here:\nCompound words: “bookkeeper” vs “book keeper” vs “book-keeper”, the same goes for a few other words, “house keeper”, “book binder”, “house wife”, etc.\nCapitalization: “At home” vs “at home”\nAbbreviations: “tel. operator”, “h.w.”\n“woman” versus “lady”: “saleslady” vs “saleswoman”, and some other titles as well\n“maker” versus “making”: any occupation ending with “-making” typically also had some cases of being specified with “-making” instead, e.g “dressmaker” vs “dressmaking”\nThe above list is pretty much just a question of spelling, and I don’t think I’m making any bold assumptions when I change these to be consistent. However, there are some other changes I am a bit less certain about that I will nonetheless make for the sake of my visualization:\nMarried or not? In many cases, they have “married” as a part of their occupational status, which isn’t relevant for my purpose. However, what does it mean if they only have “married” listed as their occupation? I have grouped this together with “housewife”, which I think is ok, but I might be wrong.\nA housewife in other words: In addition to grouping “married” and all the various spellings of “housewife” as “housewife”, I’m also assuming that “at home” and “house work” means the same thing. However, the way I understand it “housekeeper” is not the same, as they are someone doing house work in someone else’s home.\nA housekeeper in other words: I also grouped “domestic” together with housekeeper.\nNurses: There are a few different titles related to nurses. I made a decision to group together everything with the word “nurse”, though this is likely removing some nuances. Since the numbers are fairly small I feel it was ok.\nClerks: The same goes for anything clerk-related.\nIn order to make all these groupings and adjustments, I used the case_when() function from dplyr:\n\n\noccupations_clean <- voters %>% \n  mutate(occupation = tolower(occupation), # everything lowercase\n         occupation = gsub(\"-\", \"\", occupation), # remove all hyphens\n         occupation = gsub(\"\\\\[|\\\\]\", \"\", occupation), # remove all brackets\n         occupation = case_when(\n           is.na(occupation) ~ \"Not given\", \n           grepl(\"(hous.*wife)|(h. ?w)|(at home)|(^married$)|(house.*work)|(^home$)\", \n                 occupation) ~ \"housewife\",\n           grepl(\"(telephone)|(tel. operator)|(^.?phone)\", occupation) ~ \"telephone operator\",\n           grepl(\"(^m.*ch.*operat)|(operator machine)\", occupation) ~ \"machine operator\",\n           grepl(\"switch.*board\", occupation) ~ \"switchboard operator\",\n           grepl(\"(dress).*(mak)\", occupation) ~ \"dressmaker\",\n           grepl(\"sten\", occupation) ~ \"stenographer\",\n           grepl(\"boo.*keep\", occupation) ~ \"bookkeeper\",\n           grepl(\"(house.*keep)|(domestic)\", occupation) ~ \"housekeeper\",\n           grepl(\"book.*binder\", occupation) ~ \"bookbinder\",\n           grepl(\"box.*maker\", occupation) ~ \"box maker\",\n           grepl(\"brush.*maker\", occupation) ~ \"brush maker\",\n           grepl(\"nurse\", occupation) ~ \"nurse\",\n           grepl(\"store.*keeper\", occupation) ~ \"storekeeper\",\n           grepl(\"(( )|^)(hair)\", occupation) ~ \"hairdresser\",\n           grepl(\"telegraph\", occupation) ~ \"telegrapher\",\n           grepl(\"candy\", occupation) ~ \"candy worker\",\n           grepl(\"sales\", occupation) ~ \"saleslady\",\n           grepl(\"laund\", occupation) ~ \"laundress\",\n           grepl(\"fore\", occupation) ~ \"forelady\",\n           grepl(\"(office work)|(office assistant)\", occupation) ~ \"office work\",\n           grepl(\"piano\", occupation) ~ \"piano teacher\",\n           grepl(\"school.*teacher\", occupation) ~ \"teacher\",\n           grepl(\"buyer\", occupation) ~ \"buyer\",\n           grepl(\"compt\", occupation) ~ \"comptometer operator\",\n           grepl(\"secretary\", occupation) ~ \"secretary\",\n           grepl(\"(^sew)|(seamstress)\", occupation) ~ \"seamstress\",\n           grepl(\"(milliner)|(hat)\", occupation) ~ \"hatmaker\",\n           grepl(\"elevator\", occupation) ~ \"elevator operator\",\n           grepl(\"physician\", occupation) ~ \"physician\",\n           grepl(\"cler\", occupation) ~ \"clerk\",\n           TRUE ~ occupation),\n         occupation = fct_lump_min(occupation, min = 7),\n         occupation = toupper(occupation)\n         ) %>% \n  count(occupation, sort = TRUE) %>% \n  mutate(order = 1:nrow(.))\n\n\nThis was very good practice in regular expressions! My workflow for discovering all the necessary changes and how to capture them as concisely as possible in a regular expression consisted of both manual searches in View() in RStudio, as well as lots of testing in (regex101.com)[regex101.com] (what an amazing resource!). I would write a list of all the various spellings of an occupation in regex101.com, and then try to come up with an appropriate regular expression to capture all of them. I haven’t worked a ton with regular expressions before, so I suspect that some of these could have been done better, but I definitely learned quite a bit from this. It was always a challenge to make sure not to include something by mistake, for instance I thought I would just search for anything containing “hair” and recode that as hairdresser, but just searching for “hair” would also select “chairwoman”, which was not what I wanted!\nI also made the decision to group together all the occupations that came up six times or less into “other”. Though they were definitely interesting to look at! I for sure think it is possible to group the occupations a bit more, I put more than 500 unique entries into my “other” category, but after scrolling through them I think I at least managed to pick out the ones that would make a significant impact in my visualization.\nA preliminary bar chart\nI decided early on that I wanted to make this into a bar chart to make the sizes of the different occupations clear. But a simple bar-chart of this would look something like this:\n\n\nggplot(occupations_clean, aes(x = n, y = reorder(occupation, n))) +\n  geom_col() +\n  theme_minimal()\n\n\n\nThis is fine, but the size of the “housewife” category kind of masks the other categories. Is there some way of shifting the focus to the other categories while still displaying the size of the housewife-category? This made me think of some of the visualizations by W.E.B Du Bois, where I remembered seeing a “bent” bar chart, similar to a normal bar chart but with one of the bars being folded.\nA bent bar chart!\nWhen I looked it up I found that Du Bois has a few different data portraits of this type, for instance Plate 17 and Plate 26, shown below. Note that Plate 26 also displays occupations!\n\n\n\nFigure 1: caption\n\n\n\nThere is, as far as I know, no built-in way to do this in ggplot2. Originally I was thinking it would be really cool to break the bar and then let it go downwards at 90 degrees, maybe even breaking again towards the bottom, almost like a frame. Byt since the y-axis here is different from the x-axis, I would need to be very careful that the length was correct, and I think that will need to be a project for another time. To keep it manageable this time I decided to bend the bar 180 degrees.\nTo make the curve itself, I used geom_curve():\n\n\nggplot() +\n  geom_curve(aes(x = 1, y = 1, xend = 1, yend = 1+1), \n             curvature = 1.4, linewidth = 10) +\n  theme_minimal()\n\n\n\nIt took some tweaking to find the correct curvature to make the transition between the curve and the bar look smooth, but evetually curvature = 1.4 seemed to do the trick.\nI decided to break the housewife-bar into three. So first, I need to find the length of each of the sub-segments by just dividing the total number in three. I also actually make this the number of housewives in the occupations_clean data set, so the first bar will be generated the same way as all the other bars, and then I add the two other sub-sections with geom_segment().\n\n\nn_housewives <- occupations_clean[1, 2] %>% as.numeric()\nn_housewives_div3 <- round(n_housewives/3)\noccupations_clean[1, 2] <- n_housewives_div3 # Change the number of housewives to a third.\n\nn_occs <- nrow(occupations_clean) # Number of occupations\nbar_width <- 1.5 # Width of the bars\n\n# Adding this in order to place the text correctly later on:\noccupations_clean$text_position_y <- n_occs - (occupations_clean$order-1)\noccupations_clean$text_position_y[1] <- n_occs + 2\n\n\nTo make everything look the same I actually use geom_segment() for all the “normal” bars as well instead of geom_col(). So, our initial plot with only the normal bars looks like this:\n\n\noccupations_plot <- ggplot(occupations_clean) +\n  geom_segment(aes(x = 0, xend = n, \n                   y = reorder(occupation, n), yend = reorder(occupation, n)),\n               linewidth = bar_width)\noccupations_plot\n\n\n\nThough here the housewives-bar is actually just a third of what it should be. I then add the curves and segments, and expand the y-axis so the bent bar will be visible:\n\n\noccupations_plot <- occupations_plot +\n  # Bottom curve, right side\n  geom_curve(aes(x = n_housewives_div3, y = n_occs, xend = n_housewives_div3, yend = n_occs+1), \n                 colour = \"black\", curvature = 1.4, linewidth = bar_width) +\n  # Middle segment\n  geom_segment(aes(x = n_housewives_div3, xend = 0, y = n_occs+1, yend = n_occs+1), \n               linewidth = bar_width) +\n  # Top curve, left side\n  geom_curve(aes(x = 0, y = n_occs+1, xend = 0, yend = n_occs+2), \n             colour = \"black\", curvature = -1.4, linewidth = bar_width) +\n  geom_segment(aes(x = n_housewives_div3, xend = 0, y = n_occs+2, yend = n_occs+2), \n               linewidth = bar_width) +\n  scale_y_discrete(expand = expansion(mult = c(0.01, .05)))\n\noccupations_plot\n\n\n\nNice! That wasn’t too tricky! Now for adding annotations, and beautifying everything.\nI want to highlight some of the bars for emphasis, in this case I prefer doing that by defining a separate color column in the dataset.\n\n\ncol_housewife <- \"#D2042D\"\noccupations_clean <- occupations_clean %>% \n  mutate(bar_color = case_when(occupation == \"HOUSEWIFE\" ~ col_housewife,\n                               occupation %in% \n                                 c(\"NONE\", \"UNEMPLOYED\", \n                                   \"SINGLE\", \"NOT GIVEN\", \"OTHER\") ~ \"grey86\",\n                                TRUE ~ \"#D2042D\"))\n\n\n\n\nlibrary(showtext)\n\nshowtext_auto()\nshowtext_opts(dpi = 300)\n\nf1 <- \"Cabin\" # Title font\nf2 <- \"Cabin\" # Body text font\nfont_add_google(name = f1, family = f1)\nfont_add_google(name = f2, family = f2)\n\ncol_text <- \"#191919\"\ncol_bg <- \"#f9f9f7\"\n\nggplot(occupations_clean) +\n  # Main bars\n  geom_segment(aes(x = 0, xend = n, \n                   y = reorder(occupation, n), yend = reorder(occupation, n),\n                   color = I(bar_color)),\n               linewidth = bar_width) +\n  # Bottom curve, right side\n  geom_curve(aes(x = n_housewives_div3, xend = n_housewives_div3, \n                 y = n_occs, yend = n_occs+1, \n                 color = col_housewife), \n                 curvature = 1.4, linewidth = bar_width) +\n  # Middle segment\n  geom_segment(aes(x = n_housewives_div3, xend = 0, \n                   y = n_occs+1, yend = n_occs+1,\n                   color = col_housewife), \n               linewidth = bar_width) +\n  # Top curve, left side\n  geom_curve(aes(x = 0, xend = 0, \n                 y = n_occs+1, yend = n_occs+2,\n                 color = col_housewife), \n             curvature = -1.4, linewidth = bar_width) +\n  geom_segment(aes(x = n_housewives_div3, xend = 0, \n                   y = n_occs+2, yend = n_occs+2,\n                   color = col_housewife), \n               linewidth = bar_width) +\n  scale_y_discrete(expand = expansion(mult = c(0.01, .05))) +\n  # Number labels\n  geom_text(aes(x = n + 30, y = text_position_y, label = n), hjust = 0, \n            size = 2, family = f2, color = col_text) +\n  # Add a little buffer of each side of the x-axis\n  scale_x_continuous(limits = c(0, 2300), expand = expansion(mult = c(0.01, 0))) +\n  # Don't remove stuff outside the border of the plot\n  coord_cartesian(clip = \"off\") + \n  annotate(geom = \"text\", x = 2300, y = 48, hjust = 1, family = f1, face = \"bold\", color = col_text, size = 6,\n           label = \"CHERRY PICKERS\\n AND CHOCOLATE DIPPERS\") +\n  annotate(geom = \"text\", x = 2300, y = 42, hjust = 1, family = f1, color = col_text, size = 3,\n           label = \"The most common occupations among 11992 of the first\\n women who signed up to vote in Boston in 1920.\") +\n  labs(caption = \"Graphics: Emma Skarstein  |  Source: The Mary Eliza Project\") +\n  theme_minimal() +\n  theme(text = element_text(size = 20, family = f2, color = col_text),\n        plot.caption = element_text(size = 6, family = f2, color = col_text),\n        axis.text.y = element_text(size = 6, color = col_text),\n        axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        panel.grid = element_blank(),\n        panel.background = element_rect(fill = col_bg, color = col_bg),\n        plot.background = element_rect(fill = col_bg, color = col_bg),\n        plot.margin = margin(20, 20, 20, 20))\n\n\n\nAppendix: Some other exploration of the data\nThis is such a rich dataset, I wanted to take a quick look at some of the other information there as well.\nBirth countries\nFirst of all, I would definitely recommend just scrolling through this dataset, I could spend hours just looking at all the individual entries!\nWe can also do some simple summaries, for example, what are the birth countries of these women?\n\n\ncountries <- voters %>% count(country_of_birth, sort = TRUE)\ncountries %>% head(15)\n\n      country_of_birth    n\n1        United States 9559\n2               Canada  823\n3              Ireland  780\n4              England  213\n5               Russia  102\n6                 <NA>   98\n7                Italy   77\n8             Scotland   77\n9               Sweden   67\n10             Germany   62\n11              Norway   17\n12 British West Indies   16\n13              France   15\n14             Austria    8\n15             Denmark    8\n\nUnsurprisingly a majority is born in the US, but there are quite a few immigrants as well. We can print all of them without the counts to save space:\n\n\ncountries[,1]\n\n [1] \"United States\"          \"Canada\"                \n [3] \"Ireland\"                \"England\"               \n [5] \"Russia\"                 NA                      \n [7] \"Italy\"                  \"Scotland\"              \n [9] \"Sweden\"                 \"Germany\"               \n[11] \"Norway\"                 \"British West Indies\"   \n[13] \"France\"                 \"Austria\"               \n[15] \"Denmark\"                \"Belgium\"               \n[17] \"Switzerland\"            \"Wales\"                 \n[19] \"Poland\"                 \"West Indies\"           \n[21] \"[Canada]\"               \"Armenia\"               \n[23] \"Azores\"                 \"China\"                 \n[25] \"Danish West Indies\"     \"Finland\"               \n[27] \"Great Britain\"          \"Holland\"               \n[29] \"Nova Scotia\"            \"Portugal\"              \n[31] \"Russian Poland\"         \"South Wales\"           \n[33] \"Turkey\"                 \"[see note]\"            \n[35] \"Armenia (Turkey)\"       \"At sea\"                \n[37] \"Austria Czechoslovakia\" \"Austria Hungary\"       \n[39] \"Bohemia\"                \"Chile\"                 \n[41] \"Cuba\"                   \"German\"                \n[43] \"Hungary\"                \"Jamaica\"               \n[45] \"Lithuania\"              \"Massachusetts\"         \n[47] \"Newfoundland\"           \"North Germany\"         \n[49] \"Northern Ireland\"       \"Norway Sweden\"         \n[51] \"Poland-Russia\"          \"Romania\"               \n[53] \"Russia Poland\"          \"South Africa\"          \n[55] \"Spain\"                  \"Syria\"                 \n[57] \"Untied States\"          \"Vermont\"               \n\nInteresting! If we were to do some analysis based on this I think we could do some further cleaning here, though we need to be careful about making assumptions when working with historical data. For instance, I was thinking that I would change Newfoundland to Canada, but turns out Newfoundland wasn’t a part of Canada until 1949! Also, Norway separated from Sweden in 1905, but the one person who wrote Norway Sweden as their country of birth was probably born before then, when they were still a union. Also, apparently Vermont was it’s own republic from 1777 to 1791, but I’m guessing this person wasn’t born in that time period. Maybe it was a political statement? But I suppose we could assume that “Russian Poland”, “Russia Poland” and “Poland-Russia” are referring to the same place?\nPopular and rare names\nI also think name trends are really interesting, so I am curious about the most popular names:\n\n\nnames <- voters %>% \n  separate(name, into = c(\"first\", \"rest\"), sep = \"\\\\s\", extra = \"merge\") %>% \n  count(first, sort = TRUE)\n\nhead(names, n = 10)\n\n       first    n\n1       Mary 1618\n2   Margaret  614\n3  Elizabeth  436\n4      Annie  403\n5  Catherine  340\n6      Alice  295\n7      Helen  292\n8       Anna  271\n9      Sarah  206\n10     Ellen  191\n\nThese are maybe not so surprising. We can look at a sample of the least common names:\n\n\nset.seed(1920)\nspecial_names <- names %>% filter(n == 1)\nrandom_rows <- sample(1:nrow(special_names), size = 100)\nspecial_names[random_rows, 1]\n\n  [1] \"Chrystal\"     \"Susanne\"      \"Avis\"         \"Harriott\"    \n  [5] \"Youtha\"       \"Lorenna\"      \"Florance\"     \"Nichols\"     \n  [9] \"[Nano]\"       \"Carmela\"      \"Orrie\"        \"Rowena\"      \n [13] \"Lorana\"       \"Imogen\"       \"Hanorah\"      \"Alvania\"     \n [17] \"Missouri\"     \"Palmira\"      \"Jessemine\"    \"Audrey\"      \n [21] \"Juliet\"       \"Reyda\"        \"Winnie\"       \"Mignonette\"  \n [25] \"Campbell\"     \"Neotah\"       \"Catharin\"     \"Bride\"       \n [29] \"Nicoline\"     \"Margarite\"    \"Corilla\"      \"[Malhilata?]\"\n [33] \"Eloise\"       \"Ursuline\"     \"Abigale\"      \"Vyola\"       \n [37] \"Loulie\"       \"Lotty\"        \"Letha\"        \"Freda\"       \n [41] \"Mal\"          \"Jenette\"      \"Lera\"         \"Albena\"      \n [45] \"Latitia\"      \"E.D.\"         \"Argentina\"    \"Aimee\"       \n [49] \"Mannie\"       \"Hilma\"        \"Robena\"       \"Olie\"        \n [53] \"Alethia\"      \"Minne\"        \"Mahala\"       \"Tema\"        \n [57] \"Vena\"         \"Patty\"        \"Katheryne\"    \"Hilla\"       \n [61] \"Nanie\"        \"Seraph\"       \"Georgette\"    \"Cera\"        \n [65] \"Luzerne\"      \"Vida\"         \"Metta\"        \"Elinore\"     \n [69] \"Beissie\"      \"Beata\"        \"Rosabelle\"    \"Stephanie\"   \n [73] \"Ermina\"       \"India\"        \"Minnia\"       \"Monica\"      \n [77] \"Tena\"         \"Yettie\"       \"Lovetta\"      \"Eulalie\"     \n [81] \"Tryphena\"     \"Meta\"         \"Raffaelle\"    \"Leta\"        \n [85] \"Matildia\"     \"Clifford\"     \"[Mattie]\"     \"Eleanore\"    \n [89] \"Louies\"       \"Mercy\"        \"Diana\"        \"Pearle\"      \n [93] \"Henny\"        \"Elberta\"      \"Filomena\"     \"Hila\"        \n [97] \"Wanda\"        \"Tillie\"       \"Bluette\"      \"Eleana\"      \n\nI love this! I suspect some of these may be misspelled, but that is just the joy of historical data.\n\n\n\n",
    "preview": "posts/2023-02-23-what-did-women-work-with-in-1920-a-data-visualization-case-study/what-did-women-work-with-in-1920-a-data-visualization-case-study_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2023-03-15T11:03:03+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-06-10-isba-2022-poster-supplement/",
    "title": "ISBA 2022 poster supplement",
    "description": "Accompanying files to my ISBA 2022 poster presentation, presenting a joint framework for accounting for measurement error and missing data in INLA.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-06-10",
    "categories": [],
    "contents": "\nISBAposterABSTRACT\nMeasurement error and missing values in covariates are inescapable in\nany discipline that deals with data, and both problems have received\nconsiderable attention. Despite this, many applied scientists are still\nnot routinely accounting for varying types of measurement error in their\ncovariates.\nHere we describe a joint Bayesian framework that simultaneously\naccounts for measurement error and missing data, by using the fact that\nmissing data can be interpreted as an extreme case of missing data. In\naddition, we show how to account for Berkson measurement error in the\nsame framework, which provides a convenient setup for modelling Berkson\nmeasurement error, classical measurement error, missing data, or any\ncombination of these in the same or different covariates.\nBy using integrated nested Laplace approximations (INLA) we show how\npotentially complex Bayesian hierarchical models can be implemented\nefficiently. The approach is applicable to a wide variety of regression\nmodels, some of which will be exemplified in this paper using both\nsimulated and real data.\nTODO: Adjust this to the poster presentation\nTHE\nMOST GENERAL MODEL: Missing data, classical measurement error and\nBerkson measurement error\nTODO: Equations and text from the paper\nEXAMPLES\nPoster design\nMy poster design is completely copied from this Psycho poster (link).\nI have also been very inspired by Mike Morrison’s #betterposter\ncampaign, and would definitely recommend checking out his videos on how\nto make better academic posters in less time, here is one of them: https://www.youtube.com/watch?v=WBjhxjWDiHw. Of course,\nMike Morrison’s point is to help researchers spend less time on their\nposters. But I really like fiddling with the design of my poster, and so\nI tried to adapt his principles while simultaneously trying to copy the\ndesign of the movie poster. I think vintage movie posters or\nadvertisements are actually great models for how we could design\nacademic posters, as they grab your attention with impactful and\nattractive colors and fonts, while at the same time not being too\nover-crowded and often very minimalistic. I think this use of fonts,\ncolors and simplistic figures or icons can be a great guide, and copying\na movie poster makes it more of an interesting challenge rather than a\nfrustration.\nI made the poster in Power Point, using google fonts when I couldn’t\nfind a good font match in the default fonts. The equations are made\nusing eulerfont in Latex and appropriate font and background color, and\nthen screen-shotted into the poster.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-testing/",
    "title": "Testing",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-05-02",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-05-02-using-spin-to-create-manageable-supplementary-material/",
    "title": "Using spin to create manageable supplementary material",
    "description": "Good supplementary material means a lot more than just uploading your R scripts. Rmarkdown allows us to easily provide comments and explanations of our code, but it means more files to manage -- perhaps you already have the code in an R script and are manually copying it into the .Rmd-file. With knitr's spin function, you can automatically convert your R script to an Rmarkdown file, leading to easier code-sharing and ultimately making your research easier to re-do. Here, I share some of the tricks I have learned about how to make clean, user-friendly supplementary material using spin.",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2022-05-02",
    "categories": [],
    "contents": "\nThe `knitr::spin()`` function takes any r script and turns it into a html or pdf document without any further modifications. You do this either by clicking the little notebook symbol in the toolbar above your script, or by pressing shift + command + K (the same shortcut as you use to knit). However, if you want to, you may use the syntax for Roxygen comments to add normal text to the compiled document, as well as a YAML frontmatter and anything else you would to in a regular Rmarkdown document. You can read more about the details of basic spinning here: https://bookdown.org/yihui/rmarkdown-cookbook/spin.html\nIn this post I want to share some things I’ve learned from using spin to create the supplementary material for my paper.\nFile paths\nChild files\nWorkflow\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-08-power-usage-project/",
    "title": "Power consumption project",
    "description": "Through elhub.no I have access to all the power consumption data from my apartment. I have made the data available in an R package, and this is a brief look at it!",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2021-11-08",
    "categories": [],
    "contents": "\n(This post was first made in January 2021, I’m just reposting now that I have this blog thing going)\nWelcome to my power usage project! Through elhub I have downloaded (and will continue to download) monthly data sets containing the hourly power usage in my apartment. I made an R-package that contains all the data plus some useful functions. Here I will explore some visualisations of the data.\nMy goal is to produce visualizations of these data that are easy to interpret for people without any particular statistical background. I this document I will be trying out different things and showing a bit of the process, so this is not meant to be a perfect finished product, rather a kind of documentation of the process.\n\n\nShow code\n\n#devtools::install_github(\"emmaSkarstein/power.usage.analysis\")\nlibrary(power.usage.analysis)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(hrbrthemes)\nlibrary(ggridges)\nlibrary(patchwork)\nlibrary(jcolors)\nlibrary(ggpubr)\ntheme_set(theme_bw())\n\n\n\nFirst, let us load the data and look at the structure of it.\n\n\nShow code\n\ndata <- clean_and_prepare(power_data)\nhead(data)\n\n\n                     tid forbruk       date month weekday year\n...1 2019-01-14 00:00:00   0.338 2019-01-13     1  Monday 2019\n...2 2019-01-14 01:00:00   1.047 2019-01-14     1  Monday 2019\n...3 2019-01-14 02:00:00   0.327 2019-01-14     1  Monday 2019\n...4 2019-01-14 03:00:00   1.475 2019-01-14     1  Monday 2019\n...5 2019-01-14 04:00:00   2.791 2019-01-14     1  Monday 2019\n...6 2019-01-14 05:00:00   3.286 2019-01-14     1  Monday 2019\n         hour\n...1 00:00:00\n...2 01:00:00\n...3 02:00:00\n...4 03:00:00\n...5 04:00:00\n...6 05:00:00\n\nSo our data cosists of the hourly power consumption, as well as columns giving the year, date, month, weekday and hour for each observation. Now I want to explore some visualizations.\nBasic plot\nLet’s first try a simple plot of all the observations.\n\n\nShow code\n\np_simple <- ggplot(data, aes(x = tid, y = forbruk)) + \n  geom_line() +\n  ylab(\"Power usage\") +\n  xlab(\"\")\np_simple\n\n\n\n\nThis is a mess. While we do see some general tendencies that are interesting (like maximum consumption varying seasonally), it is difficult to tell what is going on in all the noise. It is also worth noting the really low values during August of 2020. This was due to some error with our power measuring device, and we had to have a guy from the company come fix it. I won’t be handling this in any special way, for now I will just treat them along with the rest of the data. It is worth noting that the values have not been set to zero or NA, instead they are just unrealistically low. If these kinds of problems with the measuring device was a common problem, this would be really frustrating, as there is no way of knowing then if the measurement is actually that low or if there is an error (although appearently we didn’t get charged less for this period, since they knew something was wrong they instead extrapolated from the previous correct values).\nAverage per weekday - Pitfalls of the boxplot\nAnother option is to try to group the data by some time period by summing or averaging it. For instance, we can look at the power usage for each weekday. For this, we might be interested in looking at the average power consuption per day, which could easily be calculated. However, just calculating the average, without showing any information about the spread or variation in the observations, is not very good. And when we also have the option to display things graphically instead of just giving summary statistics, a boxplot is a common choice. In this case, a boxplot would look like this:\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk)) +\n  geom_boxplot(fill = \"hotpink3\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  annotate(\"text\", x = 1, y = c(-0.1, 0.4, 0.75, 1.9, 4.5, 6), label = c(\"Minimum\", \"25th percentile\", \"Median\", \"75th percentile\", \"Maximum\", \"Outliers\"), size = 2.2, fontface = \"bold\", color= \"darkcyan\")\n\n\n\n\nThe boxplot displays a lot of information. The first thing I look at is the bold line in the middle of the orange box, which indicates the median (the 50th percentile) of the values in this group. The edges of the box indicate the 25th and 75th percentiles (25th percentile means the value that 25% of the observations lie below, and 75% lie above, and similarly for the 75th percentile). The ends of the “whiskers” show the minimum and maximum value of the data, excluding the outliers, and then the outliers are plotted explicitly.\n\nThe box plot (or plots with range-bars) was introduced by the data visualization specialist Mary Eleanor Spear, first in her book Charting statistics in 1952.\n\nFrom the above plot, it really doesn’t look to me like there are any real differences in power usage between each weekday. I would be ok with leaving it like this, but boxplots are a bit controversial, since they may easily show the same box plot for observations of very different distributions. See this nice article for a further discussion of this. There are some ways to fix this, however. A simple fix to make sure we aren’t hiding any important information about the distribution of the data is to plot a jittered scatterplot on top. That would look like this for us:\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk)) +\n  geom_boxplot(fill = \"darkorange\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  geom_jitter(color=\"black\", size=0.4, alpha=0.9)\n\n\n\n\nThis is quite transparent, but in our case it looks chaotic since we have so many observations. It also does’t solve the problem that the boxplot is not immediately obvious if you have never seen one before, which I would say is a major disadvantage when visualizing data for most audiences.\nAn alternative to the box-plot is the violin plot. Personally, I’m not a fan of the violin plot because I initially found the fact that it is symmetric around a center line to be a bit confusing. If I want the plot to be really easy to interpret, I find the ridge plot to be superior to both the boxplot and violin plot. Here is how it looks using the same data as above:\n\n\nShow code\n\nggplot(data, aes(x = forbruk, y = weekday)) +\n  geom_density_ridges(alpha = 0.8, fill = \"hotpink3\") +\n  theme_ridges() +\n  ylab(\"\") +\n  xlab(\"Power usage\")\n\n\n\n\n—-some interpretation of this—-\nAs a last point I just want to compare the boxplot, violin plot and ridge plot here for the reader, because I think preferences vary and it is interesting seeing the different options (note that I’ve flipped the axes on the two first so they are the same direction as the ridge plot).\n\n\nShow code\n\np_box <- ggplot(data, aes(y = weekday, x = forbruk)) +\n  geom_boxplot(fill = \"darkorange2\") +\n  xlab(\"\") +\n  ylab(\"\")\n\np_violin <-ggplot(data, aes(y = weekday, x = forbruk)) +\n  geom_violin(fill = \"hotpink3\") +\n  xlab(\"\") +\n  ylab(\"\") + \n  theme(axis.text.y = element_blank())\n\np_ridges <- ggplot(data, aes(x = forbruk, y = weekday)) +\n  geom_density_ridges(alpha = 0.8, fill = \"darkgreen\") +\n  ylab(\"\") +\n  xlab(\"\") +\n  theme(axis.text.y = element_blank())\n\n(p_box + p_violin + xlab(\"Power usage\") + p_ridges) \n\n\n\n\nOkay, it pretty much looks like the distribution is the same for each week day, maybe not surprisingly, although I might have expected the consumption to be a bit higher during the week-end. Really kind of boring.\nDid the pandemic affect power usage for each weekday?\nHowever, during most of 2020 we worked from home. Could it be that this is affecting the results, and that weekday differences were larger in 2019? Let’s investigate.\n\n\nShow code\n\nggplot(data, aes(x = weekday, y = forbruk, fill = year)) +\n  geom_boxplot() +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\nThere are some differences here, but I really don’t think we can say for certain that there is a meaningful difference between the power consumption in 2019 and 2020 for any of the weekdays. This makes sense, because although we were probably more outside of the appartment in 2019, usually there would be at least one of us at home regardless, so the situation is not very different.\nTotal per month\nInspired by the weekday plots, I want to examine if the total monthly power consumption has changed between 2019 and 2020. Instead of looking at the median, average or some other measure per month, I will simply look at the total power consumption per month in 2019, and separately in 2020, since a month is a continuous stretch of time. Had I been looking at monthly consumption not separated by year, however, I would have calculated the average total monthly consumption across those years. Maybe that will be interesting once I have data from more than just two years.\nAnyway – here is the total power usage per month for 2019 and 2020. This is also the plot that our power provider shows me on their webpage, and I feel like it is particularly useful for just seeing how much we use. Of course – some months are longer than others, so it is not completely fair to compare between the months, but it certainly gives a rough impression.\n\n\nShow code\n\nyear_month <- data %>%\n  group_by(month, year) %>%\n  summarise(sum_forb = sum(forbruk))\n\nggplot(year_month, aes(x = month, y = sum_forb)) +\n  geom_col(aes(fill = year), color = \"black\", position = \"dodge\") +\n  scale_fill_jcolors(palette = \"pal6\") +\n  ylab(\"Total power usage per month\") +\n  xlab(\"\")\n\n\n\n\nHere we can sort of see the seasonal trend, and it seems very reasonable to me. I find it interesting that March is the month with the highest power consumption, I would not have guessed that in advance.\nDaily usage\nThere is another resolution between our first plot and the monthly total plot in the previous section: it might also be interesting to look at the total daily consumption. I expect this will be slightly smoother than the first plot, and it may be interesting to see the time-series day-by-day instead of grouped by month like above.\n\n\nShow code\n\ndaily <- data %>%\n  group_by(date) %>%\n  summarise(sum_forb = sum(forbruk))\n\nggplot(daily, aes(x = date, y = sum_forb)) +\n  geom_point(size = 0.5) +\n  geom_line(alpha = 0.5, color = \"hotpink4\") +\n  ylab(\"Power usage\") +\n  xlab(\"\")\n\n\n\n\nHere we see more of the variation between different days in the same month. I would really have liked to know what happened on those peak days.\nUsage throughout the day\nAs a final plot, I would like to know how the power usage is distributed throughout the day. First just a simple box plot:\n\n\nShow code\n\nggplot(data, aes(x = hour, y = forbruk)) +\n  geom_boxplot(fill = \"hotpink3\") +\n  ylab(\"Power usage\") +\n  xlab(\"\") +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nI also want to try a fancier, less detailed version that represents time periods with higher usage as larger areas. I have split the day into 3-hour intervals (should I use a finer resolution?), and the below plot shows the total power consumption in each time period. (Would it be better with a donut to represent the day as a cycle?) (would be nice to show the times on the plot somehow)\n\n\nShow code\n\n# Adding grouped time column\ndata <- data %>% mutate(grouped_time = \n                          factor(floor(as.numeric(substr(hour, start = 1, stop = 2))/3)))\n\nggplot(data, aes(x = 1, y = -forbruk, fill = grouped_time)) +\n  geom_col() +\n  scale_fill_brewer(labels = c(\"00:00 - 02:00\", \"03:00 - 05:00\", \n                                 \"06:00 - 08:00\", \"09:00 - 11:00\", \n                                 \"12:00 - 14:00\", \"15:00 - 17:00\",\n                                \"18:00 - 20:00\", \"21:00 - 23:00\"),\n                    palette = \"BrBG\") +\n  theme_transparent() +\n  coord_flip() +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-11-08-power-usage-project/power-usage-project_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-28-ways-of-visualizing-missing-data/",
    "title": "Ways of visualizing missing data",
    "description": "When recently looking at a data set with a lot of missing data I tried out a few different ways of quickly summarizing the missingness in the different variables. Here is a brief guide to the visualizations I found the most useful!",
    "author": [
      {
        "name": "Emma Skarstein",
        "url": "https://emmaskarstein.github.io"
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\nFor this demonstration, we will borrow datasets from the package mice.\n\n\nShow code\n\nlibrary(mice)\nlibrary(tidyverse)\nlibrary(patchwork) # Combining plots\nlibrary(showtext) # Font\nlibrary(ggthemes) # Color palettes (I use the canva palettes here)\nlibrary(naniar) # Upset plots for missing values in ggplot2\nlibrary(eulerr) # Euler diagrams, proportional to size\nlibrary(ggforce) # Ellipses in ggplot\nlibrary(gt) # Great tables\nlibrary(gtExtras) # Extra stuff for the tables\nlibrary(scales) # For wrapping axis text\nlibrary(paletteer) # color palettes (I used a fish one)\n\n\n\nFirst example: Mammal sleep\nOur first data set is a set of various sleep characteristics of 62 mammals.\n\n\nShow code\n\ndata(mammalsleep)\n\ndata <- mammalsleep\n\nbetter_names <- c(Species = \"species\", Body_weight = \"bw\", Brain_weight = \"brw\", \n                  Slow_wave_sleep = \"sws\", Paradoxical_sleep = \"ps\", \n                  Total_sleep = \"ts\", Maximum_life_span = \"mls\", \n                  Gestation_time = \"gt\", Predation_index = \"pi\", \n                  Sleep_exposure_index = \"sei\", Overall_danger_index = \"odi\")\n\ndata <- data %>% rename(all_of(better_names))\nnames(data) <- gsub(\"_\", \" \", names(data), fixed=TRUE)\n\n\n# Font\nf1 <- \"Open Sans\"\nfont_add_google(name = f1, family = f1)\nshowtext_auto()\n\n# Colors\npal <- paletteer_d(\"fishualize::Halichoeres_radiatus\")\n\n\n\nDirect plotting of missingness: A rowplot\n\n\nShow code\n\nmissing_row.plot <- data %>%\n  mutate(id = row_number()) %>%\n  gather(-id, key = \"key\", value = \"val\") %>%\n  mutate(isna = is.na(val)) %>%\n  ggplot(aes(key, id, fill = isna)) +\n  geom_raster(alpha=0.82) +\n  scale_fill_manual(name = \"\",\n                    values = pal[c(1,3)],\n                    labels = c(\"Present\", \"Missing\")) +\n  scale_x_discrete(position = \"top\", labels = wrap_format(10), expand = c(0,0)) +\n  scale_y_continuous(breaks = c(1, seq(10, nrow(data), by = 10)), expand = c(0,0), trans = \"reverse\") +\n  labs(x = \"\",\n       y = \"Row Number\") +\n  theme_bw() +\n  theme(legend.position = \"top\",\n        text = element_text(size = 15, family = f1),\n        panel.grid = element_blank())\n\nmissing_row.plot\n\n\n\n\nSummary table\nNext, I just want a table that summarizes the number of missing observations in each variable. For this, I will first make a data frame with the counts and percentages of missing in each variable:\n\n\nmissing_count <- data %>% is.na %>% as.data.frame() %>% \n      map_int(sum) %>% as.data.frame() \nmissing_count$variable = rownames(missing_count)\nmissing_count <- missing_count %>% rename(count = \".\") %>% \n  mutate(percent = 100*count/nrow(data)) %>% \n  relocate(variable)\n\n\n\nThen we can easily make this into a nice table using the gt package.\n\n\nShow code\n\n# Table -----\ncolumns_with_missing <- missing_count %>% \n  filter(count > 0) %>% \n  dplyr::select(variable) %>% \n  as.matrix()\n\nmissing_table <- missing_count %>% \n  dplyr::filter(variable %in% columns_with_missing) %>% # select only columns that have missing data\n  rename(Variable = \"variable\", \"# missing\" = count, \"% missing\" = percent) %>% # rename columns\n  gt() %>% # table\n  gt_plt_bar_pct(column = \"% missing\", scaled = TRUE, fill = pal[4]) %>%  # make the percentage column into a barchart\n  tab_style(style = cell_text( # change font\n    font = google_font(f1)), \n    locations = list(cells_column_labels(everything()), \n                     cells_body(columns = c(1,2))))\n\nmissing_table\n\n\n\nVariable\n      # missing\n      % missing\n    Slow wave sleep\n14\nParadoxical sleep\n12\nTotal sleep\n4\nMaximum life span\n4\nGestation time\n4\n\n\nEuler diagram\nThe row-plot and table are both great for getting a quick overview of the data and the number of missing values. But especially with the table, we have no information about the interactions in the missingness, that is, are many of the missing values in the same row? We see this to some degree in the row-plot, but in this case we only have 62 observations. When the number of observations increases it becomes less clear when the missingness is in the same row. Toillustrate the overlaps in the missingness, I thought it would be illustrative with some kind of venn-diagram (I learned that the correct term for the type of plot that doesn’t show overlaps when the set is null is called an Euler diagram). I also wanted the size of the circles and overlaps to be proportional to the overlaps and number of missing observations. I found what I wanted in the package eulerr. There is a built-in-way to plot the resulting Euler diagrams, but I wanted to do it with ggplot2for a bit more freedom. It wasn’t too hard to extract the necessary numbers from the eulerr object (with good help from this vignette), and for plotting the ellipses themselves I use the ggforce package.\n\n\nShow code\n\n# Euler plot ------\neuler_mat <- data %>% is.na() %>% as.data.frame() %>% \n  dplyr::select(columns_with_missing[1:5]) \n\neuler_fit <- euler(euler_mat)\n\nellipses <- euler_fit$ellipses %>% mutate(variable = rownames(euler_fit$ellipses))\n\nmissing_euler <- ggplot(ellipses) +\n  geom_ellipse(aes(x0 = h, y0 = k, a = a, b = b, angle = phi, fill = variable), alpha = 0.5) +\n  scale_fill_manual(values = pal) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.title = element_blank())\n\nmissing_euler\n\n\n\n\nI really like the way this looks, but unfortunately it isn’t exact, especially when there are so few observations. For example, from this diagram it looks like there would be an observation that is missing the “Paradoxical sleep” measurement, but not the “Slow wave sleep”, due to the tiny un-overlapped sliver on the left. However, looking at the row-plot, we see that the set of animals missing values in “Paradoxical sleep”, is completely contained in the set of animals missing values in “Slow wave sleep”. The eulerr object gives us an overview over both the true counts in each set and the fitted values, and these could also be plotted on top of the circles, but I won’t do this here since there are so many intersections. Unfortunately, though the idea is fun, I don’t think this visualization will work very well in many cases.\n\n\neuler_fit$original.values[1:15]\n\n\n                    Slow wave sleep \n                                  0 \n                  Paradoxical sleep \n                                  0 \n                        Total sleep \n                                  0 \n                  Maximum life span \n                                  2 \n                     Gestation time \n                                  3 \n  Slow wave sleep&Paradoxical sleep \n                                  9 \n        Slow wave sleep&Total sleep \n                                  2 \n  Slow wave sleep&Maximum life span \n                                  0 \n     Slow wave sleep&Gestation time \n                                  0 \n      Paradoxical sleep&Total sleep \n                                  0 \nParadoxical sleep&Maximum life span \n                                  0 \n   Paradoxical sleep&Gestation time \n                                  0 \n      Total sleep&Maximum life span \n                                  0 \n         Total sleep&Gestation time \n                                  0 \n   Maximum life span&Gestation time \n                                  1 \n\nUpSet plot\nThere is another option that solves the problem of the impreciseness of the euler plot. I was first a little skeptical of this one just because I don’t think it is completely self-explanatory, and I think in most contexts, getting an overview of the missingness is something you want to do quick and dirty, and if visualizations are necessary you want them to be super intuitive. But this one is more precise than the Euler diagram and also shows the interactions, so to an audience that is already familiar with them (and maybe with some helpful annotations), I think it can be really useful. The plot is called an upset plot, and can be used as an alternative to Venn diagrams in other cases than just visualizing missingness. I use the implementation from the library naniar (which has several other useful functions for these types of things!)\n\n\nShow code\n\ngg_miss_upset(data, sets.bar.color = pal[1], main.bar.color = pal[4])\n\n\n\n\nThe bars on the left show the total number of missing values for each of the variables, and the vertical bars show the numbers missing in each intersection. My main complaint here is that although the documentation says that it returns a ggplot visualization, I don’t seem to be able to edit it using my typical ggplot ways, to change the color I instead had to use the arguments from UpSetR::upset.\nFor the project that motivated me to write this post, I made my own upset plot in ggplot2, but it is kind of hard-coded and the code is specific to that data. The approach in itself wasn’t too complicated though, basically you just convert the data to a data frame of the same size, but with true/false values indicating whether each observation is missing or not. Then it takes some manipulation to make the counts for each of the sets and interactions, and then the plots themselves are just standard bar charts. I also just used a dot-plot for the table-part of it, and then combined them all using patchwork (the package). If I’m able to generalize the code I will happily share it at a later point.\nAnother example\nAs another example, let’s look at this data set from mice with self-reported height and weight data from two studies, containing 2060 observations. A description of the data can be found by ?mice::selfreport.\n\n\nShow code\n\ndata(\"selfreport\")\n\ndata <- selfreport\n\n# Colors\npal <- paletteer_d(\"fishualize::Lutjanus_sebae\")\n\n\n\n\n\nShow code\n\nmissing_row.plot <- data %>%\n  mutate(id = row_number()) %>%\n  gather(-id, key = \"key\", value = \"val\") %>%\n  mutate(isna = is.na(val)) %>%\n  ggplot(aes(key, id, fill = isna)) +\n  geom_raster(alpha=0.82) +\n  scale_fill_manual(name = \"\",\n                    values = pal[c(1,4)],\n                    labels = c(\"Present\", \"Missing\")) +\n  scale_x_discrete(position = \"top\", labels = wrap_format(10), expand = c(0,0)) +\n  scale_y_continuous(breaks = c(1, seq(100, nrow(data), by = 100)), expand = c(0,0), trans = \"reverse\") +\n  labs(x = \"\",\n       y = \"Row Number\") +\n  theme_bw() +\n  theme(legend.position = \"top\",\n        text = element_text(size = 15, family = f1),\n        panel.grid = element_blank())\n\nmissing_row.plot\n\n\n\n\n\n\nShow code\n\nmissing_count <- data %>% is.na %>% as.data.frame() %>% \n      map_int(sum) %>% as.data.frame() \nmissing_count$variable = rownames(missing_count)\nmissing_count <- missing_count %>% rename(count = \".\") %>% \n  mutate(percent = 100*count/nrow(data)) %>% \n  relocate(variable)\n\n\n\n\n\nShow code\n\n# Table -----\ncolumns_with_missing <- missing_count %>% \n  filter(count > 0) %>% \n  dplyr::select(variable) %>% \n  as.matrix()\n\nmissing_table <- missing_count %>% \n  dplyr::filter(variable %in% columns_with_missing) %>% # select only columns that have missing data\n  rename(Variable = \"variable\", \"# missing\" = count, \"% missing\" = percent) %>% # rename columns\n  gt() %>% # table\n  gt_plt_bar_pct(column = \"% missing\", scaled = TRUE, fill = pal[4]) %>%  # make the percentage column into a barchart\n  tab_style(style = cell_text( # change font\n    font = google_font(f1)), \n    locations = list(cells_column_labels(everything()), \n                     cells_body(columns = c(1,2))))\n\nmissing_table\n\n\n\nVariable\n      # missing\n      % missing\n    hm\n803\nwm\n803\nprg\n1657\nedu\n1257\netn\n1257\nbm\n803\n\n\n\n\nShow code\n\n# Euler plot ------\neuler_mat <- data %>% is.na() %>% as.data.frame() %>% \n  dplyr::select(columns_with_missing[1:5]) \n\neuler_fit <- euler(euler_mat)\n\nellipses <- euler_fit$ellipses %>% mutate(variable = rownames(euler_fit$ellipses))\n\nmissing_euler <- ggplot(ellipses) +\n  geom_ellipse(aes(x0 = h, y0 = k, a = a, b = b, angle = phi, fill = variable), alpha = 0.5) +\n  scale_fill_manual(values = pal) +\n  coord_fixed() +\n  theme_void() +\n  theme(legend.title = element_blank())\n\nmissing_euler\n\n\n\n\n(Actually, it looks like eulerr just displays five sets, while we here have six variables with missing values, so this isn’t great.)\n\n\nShow code\n\ngg_miss_upset(data, sets.bar.color = pal[2], main.bar.color = pal[4])\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-10-28-ways-of-visualizing-missing-data/ways-of-visualizing-missing-data_files/figure-html5/figure.rowplot-1.png",
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-10-28",
    "categories": [],
    "contents": "\n\n\n\n",
    "preview": {},
    "last_modified": "2022-11-01T14:40:50+01:00",
    "input_file": {}
  }
]
